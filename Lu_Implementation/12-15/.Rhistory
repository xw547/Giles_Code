source("~/Working/Ning/Giles_Project_1/Code/Lu_Implementation/11-02/empirical_weight.R")
library(doParallel)
library(foreach)
registerDoParallel(7)
set.seed(2023)
n    = 2000
rho  = 0.5
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X = cbind(X_12, X_310)
colnames(X) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y = c(X%*%beta + rnorm(n, 0, 1))
reduced_model = randomForest(X[,-1], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
full_model    = randomForest(X[,], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
# Speeeeeed reduced_den = empirical_rf_pdf(reduced_model, X[,-1], X[,-1])
set.seed(2023+347)
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X_2 = cbind(X_12, X_310)
colnames(X_2) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y_2 = c(X_2%*%beta + rnorm(n, 0, 1))
### We'll start with the cpi of the first feature.
full_model_lm = lm(Y~0+X)
reduced_model_lm = lm(Y~0+X[,-1])
N = 200
zero_term = c()
first_term = c()
second_term = c()
third_term = c()
for (i in 1:n){
beta_curr = beta[-1]
beta_curr[1] = (1+rho)*beta_curr[1]
curr_y = rnorm(N, beta_curr%*%X_2[i,-1],sqrt(2-rho^2))
curr_x = rnorm(N, X_2[i,2]*rho, sqrt(1-rho^2))
second_data =  as.data.frame(cbind(curr_x, matrix(rep(X_2[i,-1], N), ncol = 9, byrow = T)))
colnames(second_data) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
zero_term[i] = 2*mean((predict(reduced_model, X_2[i,-1]) - predict(full_model, X_2[i,]))*
(Y_2[i] - predict(full_model, X_2[i,])))
first_term[i] = mean((predict(reduced_model, X_2[i,-1]) - predict(full_model, X_2[i,]))^2)
second_term[i] = mean((predict(reduced_model, X_2[i,-1]) - predict(full_model, newdata = second_data))^2)
third_term[i] = mean((Y_2[i] -  predict(full_model, newdata = second_data))^2)
}
mean(zero_term)
mean(first_term)
mean(second_term)
mean(third_term)
1-rho^2
### In this document, I'll present some results from the large sample illustration
### of exploration1, where we used lm to estimate \hat{y}
library("ggplot2")
setwd("~/Working/Ning/Giles_Project_1/Code/Lu_Implementation/12-15/")
explor2_rho1 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho1.csv"))[,-c(1,6,7)]
explor2_rho3 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho3.csv"))[,-c(1,6,7)]
explor2_rho5 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho5.csv"))[,-c(1,6,7)]
explor2_rho7 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho7.csv"))[,-c(1,6,7)]
explor2_rho9 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho9.csv"))[,-c(1,6,7)]
bias_estimation <- rbind(colMeans(explor2_rho1), colMeans(explor2_rho3),
colMeans(explor2_rho5), colMeans(explor2_rho7),
colMeans(explor2_rho9))
bias_estimation <- cbind(bias_estimation, rowSums(bias_estimation) - 2*bias_estimation[,3] - bias_estimation[,1])
bias_estimation <- cbind(bias_estimation, 3-2*(seq(1,9,2)/10)^2, (seq(1,9,2)/10))
bias_estimation <- data.frame(bias_estimation)
colnames(bias_estimation) <- c("First_Term", "Second_Term", "Third_Term",
"Forth_Term", "Estimate", "True_Value", "rho")
estimation_plot = ggplot(data = bias_estimation, aes(x = rho))   +
geom_line(aes(x = rho, y = Second_Term), color = "#D16103") +
geom_line(aes(x = rho, y = Third_Term), color = "#52854C")  +
geom_line(aes(x = rho, y = Forth_Term), color = "#4E84C4")  +
geom_line(aes(x = rho, y = True_Value), color = "#999999")
estimation_plot = ggplot(data = bias_estimation, aes(x = rho)) +
geom_line(aes(x = rho, y = True_Value), color = "#999999") +
geom_line(aes(x = rho, y = Estimate), color = "#CC79A7")
################################################################################
################################################################################
################################################################################
t_95 = qt(.975, 2000-1)
estimate_fun <- function(mat){
return(rowSums(mat) - 2*mat[,3] - mat[,1])
}
eif_mean = 3-2*seq(from =.1, to = .9, by = .2)^2
var_explor2_rho1 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho1.csv"))[, 7]
var_explor2_rho3 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho3.csv"))[, 7]
var_explor2_rho5 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho5.csv"))[, 7]
var_explor2_rho7 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho7.csv"))[, 7]
var_explor2_rho9 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho9.csv"))[, 7]
coverage_mat <- cbind(var_explor2_rho1, var_explor2_rho3, var_explor2_rho5,
var_explor2_rho7, var_explor2_rho9)
estimate_mat <- cbind(estimate_fun(explor2_rho1), estimate_fun(explor2_rho3),
estimate_fun(explor2_rho5), estimate_fun(explor2_rho7),
estimate_fun(explor2_rho9))
SE_vec <- sqrt(colMeans(coverage_mat)/1999)
coverage <- c()
for(i in 1:5){
eif_Lower = eif_mean[i] - t_95*SE_vec[i]/2
eif_Upper = eif_mean[i] + t_95*SE_vec[i]/2
eif_covered =  mean(estimate_mat[,i]<= eif_Upper&estimate_mat[,i]>=eif_Lower)
coverage[i]  = eif_covered
}
coverage
bias_estimation <- cbind(bias_estimation, coverage)
coverage_plot = ggplot(data = bias_estimation, aes(x = rho)) +
geom_line(aes(x = rho, y = coverage), color = "#4E84C4") +
geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") + ylim(0,1)
estimation_plot
coverage_plot
### The idea would be to use sample splitting
### technique to reduce the bias.
### Loading required packages
library("MASS")
library("randomForest")
library("MASS")
source("~/Working/Ning/Giles_Project_1/Code/Lu_Implementation/11-02/empirical_weight.R")
library(doParallel)
library(foreach)
registerDoParallel(7)
set.seed(2023)
n    = 2000
rho  = 0.5
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X = cbind(X_12, X_310)
colnames(X) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y = c(X%*%beta + rnorm(n, 0, 1))
reduced_model = randomForest(X[,-1], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
full_model    = randomForest(X[,], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
# Speeeeeed reduced_den = empirical_rf_pdf(reduced_model, X[,-1], X[,-1])
set.seed(2023+347)
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X_2 = cbind(X_12, X_310)
colnames(X_2) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y_2 = c(X_2%*%beta + rnorm(n, 0, 1))
### We'll start with the cpi of the first feature.
full_model_lm = lm(Y~0+X)
reduced_model_lm = lm(Y~0+X[,-1])
N = 200
beta_curr = beta[-1]
beta_curr[1] = (1+rho)*beta_curr[1]
curr_y = rnorm(N, beta_curr%*%X_2[i,-1],sqrt(2-rho^2))
curr_x = rnorm(N, X_2[i,2]*rho, sqrt(1-rho^2))
second_data =  as.data.frame(cbind(curr_x, matrix(rep(X_2[i,-1], N), ncol = 9, byrow = T)))
colnames(second_data) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
zero_term[i] = 2*mean((curr_y - X_2[i,]%*%beta)*
(Y_2[i] - X_2[i,]%*%beta))
first_term[i] = mean((curr_y  - X_2[i,]%*%beta)^2)
second_term[i] = mean((curr_y - second_data%*%beta))
i = 1
beta_curr = beta[-1]
beta_curr[1] = (1+rho)*beta_curr[1]
curr_y = rnorm(N, beta_curr%*%X_2[i,-1],sqrt(2-rho^2))
curr_x = rnorm(N, X_2[i,2]*rho, sqrt(1-rho^2))
second_data =  as.data.frame(cbind(curr_x, matrix(rep(X_2[i,-1], N), ncol = 9, byrow = T)))
colnames(second_data) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
zero_term[i] = 2*mean((curr_y - X_2[i,]%*%beta)*
(Y_2[i] - X_2[i,]%*%beta))
first_term[i] = mean((curr_y  - X_2[i,]%*%beta)^2)
second_term[i] = mean((curr_y - second_data%*%beta))
dim(second_data)
beta
second_data%*%beta
second_data =  as.matrix(cbind(curr_x, matrix(rep(X_2[i,-1], N), ncol = 9, byrow = T)))
beta_curr = beta[-1]
beta_curr[1] = (1+rho)*beta_curr[1]
curr_y = rnorm(N, beta_curr%*%X_2[i,-1],sqrt(2-rho^2))
curr_x = rnorm(N, X_2[i,2]*rho, sqrt(1-rho^2))
second_data =  as.matrix(cbind(curr_x, matrix(rep(X_2[i,-1], N), ncol = 9, byrow = T)))
colnames(second_data) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
zero_term[i] = 2*mean((curr_y - X_2[i,]%*%beta)*
(Y_2[i] - X_2[i,]%*%beta))
first_term[i] = mean((curr_y  - X_2[i,]%*%beta)^2)
second_term[i] = mean((curr_y - second_data%*%beta))
third_term[i] = mean((Y_2[i] - second_data%*%beta))
### The idea would be to use sample splitting
### technique to reduce the bias.
### Loading required packages
library("MASS")
library("randomForest")
library("MASS")
source("~/Working/Ning/Giles_Project_1/Code/Lu_Implementation/11-02/empirical_weight.R")
library(doParallel)
library(foreach)
registerDoParallel(7)
set.seed(2023)
n    = 2000
rho  = 0.5
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X = cbind(X_12, X_310)
colnames(X) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y = c(X%*%beta + rnorm(n, 0, 1))
reduced_model = randomForest(X[,-1], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
full_model    = randomForest(X[,], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
# Speeeeeed reduced_den = empirical_rf_pdf(reduced_model, X[,-1], X[,-1])
set.seed(2023+347)
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X_2 = cbind(X_12, X_310)
colnames(X_2) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y_2 = c(X_2%*%beta + rnorm(n, 0, 1))
### We'll start with the cpi of the first feature.
full_model_lm = lm(Y~0+X)
reduced_model_lm = lm(Y~0+X[,-1])
N = 200
zero_term = c()
first_term = c()
second_term = c()
third_term = c()
for (i in 1:n){
beta_curr = beta[-1]
beta_curr[1] = (1+rho)*beta_curr[1]
curr_y = rnorm(N, beta_curr%*%X_2[i,-1],sqrt(2-rho^2))
curr_x = rnorm(N, X_2[i,2]*rho, sqrt(1-rho^2))
second_data =  as.matrix(cbind(curr_x, matrix(rep(X_2[i,-1], N), ncol = 9, byrow = T)))
colnames(second_data) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
zero_term[i] = 2*mean((curr_y - X_2[i,]%*%beta)*
(Y_2[i] - X_2[i,]%*%beta))
first_term[i] = mean((curr_y  - X_2[i,]%*%beta)^2)
second_term[i] = mean((curr_y - second_data%*%beta))
third_term[i] = mean((Y_2[i] - second_data%*%beta))
}
mean(zero_term)
mean(first_term)
mean(second_term)
mean(third_term)
rho
second_data%*%beta
curr_y
### The idea would be to use sample splitting
### technique to reduce the bias.
### Loading required packages
library("MASS")
library("randomForest")
library("MASS")
source("~/Working/Ning/Giles_Project_1/Code/Lu_Implementation/11-02/empirical_weight.R")
library(doParallel)
library(foreach)
registerDoParallel(7)
set.seed(2023)
n    = 2000
rho  = 0.5
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X = cbind(X_12, X_310)
colnames(X) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y = c(X%*%beta + rnorm(n, 0, 1))
reduced_model = randomForest(X[,-1], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
full_model    = randomForest(X[,], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
# Speeeeeed reduced_den = empirical_rf_pdf(reduced_model, X[,-1], X[,-1])
set.seed(2023+347)
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X_2 = cbind(X_12, X_310)
colnames(X_2) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y_2 = c(X_2%*%beta + rnorm(n, 0, 1))
### We'll start with the cpi of the first feature.
full_model_lm = lm(Y~0+X)
reduced_model_lm = lm(Y~0+X[,-1])
N = 200
zero_term = c()
first_term = c()
second_term = c()
third_term = c()
for (i in 1:n){
beta_curr = beta[-1]
beta_curr[1] = (1+rho)*beta_curr[1]
curr_y = rnorm(N, beta_curr%*%X_2[i,-1],sqrt(2-rho^2))
curr_x = rnorm(N, X_2[i,2]*rho, sqrt(1-rho^2))
second_data =  as.matrix(cbind(curr_x, matrix(rep(X_2[i,-1], N), ncol = 9, byrow = T)))
colnames(second_data) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
zero_term[i] = 2*mean((curr_y - X_2[i,]%*%beta)*
(Y_2[i] - X_2[i,]%*%beta))
first_term[i] = mean((curr_y  - X_2[i,]%*%beta)^2)
second_term[i] = mean((curr_y - second_data%*%beta)^2)
third_term[i] = mean((Y_2[i] - second_data%*%beta)^2)
}
mean(zero_term)
mean(first_term)
mean(second_term)
mean(third_term)
3-2*rho^2
mean(rnorm(200)*rnorm(200))
mean(rnorm(200)*rnorm(200))
mean(rnorm(200)*rnorm(200))
mean(rnorm(200)*rnorm(200))
mean(rnorm(200)*rnorm(200))
mean(rnorm(200)^2)
### In this document, I'll present some results from the large sample illustration
### of exploration1, where we used lm to estimate \hat{y}
library("ggplot2")
setwd("~/Working/Ning/Giles_Project_1/Code/Lu_Implementation/12-15/")
explor2_rho1 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho1.csv"))[,-c(1,6,7)]
explor2_rho3 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho3.csv"))[,-c(1,6,7)]
explor2_rho5 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho5.csv"))[,-c(1,6,7)]
explor2_rho7 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho7.csv"))[,-c(1,6,7)]
explor2_rho9 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho9.csv"))[,-c(1,6,7)]
bias_estimation <- rbind(colMeans(explor2_rho1), colMeans(explor2_rho3),
colMeans(explor2_rho5), colMeans(explor2_rho7),
colMeans(explor2_rho9))
bias_estimation <- cbind(bias_estimation, rowSums(bias_estimation) - 2*bias_estimation[,3] - bias_estimation[,1])
bias_estimation <- cbind(bias_estimation, 3-2*(seq(1,9,2)/10)^2, (seq(1,9,2)/10))
bias_estimation <- data.frame(bias_estimation)
colnames(bias_estimation) <- c("First_Term", "Second_Term", "Third_Term",
"Forth_Term", "Estimate", "True_Value", "rho")
estimation_plot = ggplot(data = bias_estimation, aes(x = rho))   +
geom_line(aes(x = rho, y = Second_Term), color = "#D16103") +
geom_line(aes(x = rho, y = Third_Term), color = "#52854C")  +
geom_line(aes(x = rho, y = Forth_Term), color = "#4E84C4")  +
geom_line(aes(x = rho, y = True_Value), color = "#999999")
estimation_plot = ggplot(data = bias_estimation, aes(x = rho)) +
geom_line(aes(x = rho, y = True_Value), color = "#999999") +
geom_line(aes(x = rho, y = Estimate), color = "#CC79A7")
################################################################################
################################################################################
################################################################################
t_95 = qt(.975, 2000-1)
estimate_fun <- function(mat){
return(rowSums(mat) - 2*mat[,3] - mat[,1])
}
eif_mean = 3-2*seq(from =.1, to = .9, by = .2)^2
var_explor2_rho1 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho1.csv"))[, 7]
var_explor2_rho3 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho3.csv"))[, 7]
var_explor2_rho5 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho5.csv"))[, 7]
var_explor2_rho7 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho7.csv"))[, 7]
var_explor2_rho9 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho9.csv"))[, 7]
coverage_mat <- cbind(var_explor2_rho1, var_explor2_rho3, var_explor2_rho5,
var_explor2_rho7, var_explor2_rho9)
estimate_mat <- cbind(estimate_fun(explor2_rho1), estimate_fun(explor2_rho3),
estimate_fun(explor2_rho5), estimate_fun(explor2_rho7),
estimate_fun(explor2_rho9))
SE_vec <- sqrt(colMeans(coverage_mat)/1999)
coverage <- c()
for(i in 1:5){
eif_Lower = eif_mean[i] - t_95*SE_vec[i]/2
eif_Upper = eif_mean[i] + t_95*SE_vec[i]/2
eif_covered =  mean(estimate_mat[,i]<= eif_Upper&estimate_mat[,i]>=eif_Lower)
coverage[i]  = eif_covered
}
coverage
bias_estimation <- cbind(bias_estimation, coverage)
coverage_plot = ggplot(data = bias_estimation, aes(x = rho)) +
geom_line(aes(x = rho, y = coverage), color = "#4E84C4") +
geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") + ylim(0,1)
estimation_plot
bias_estimation
### The idea would be to use sample splitting
### technique to reduce the bias.
### Loading required packages
library("MASS")
library("randomForest")
library("MASS")
source("~/Working/Ning/Giles_Project_1/Code/Lu_Implementation/11-02/empirical_weight.R")
library(doParallel)
library(foreach)
registerDoParallel(7)
set.seed(2023)
n    = 2000
rho  = 0.5
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X = cbind(X_12, X_310)
colnames(X) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y = c(X%*%beta + rnorm(n, 0, 1))
reduced_model = randomForest(X[,-1], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
full_model    = randomForest(X[,], Y, nodesize = 5,
ntree = 500, keep.inbag = T)
# Speeeeeed reduced_den = empirical_rf_pdf(reduced_model, X[,-1], X[,-1])
set.seed(2023+347)
X_12 = mvrnorm(n, rep(0.5,2), Sigma = matrix(c(1, rho, rho,1), ncol =2))
# X_12 = rbinormcop(n, rho)
X_310 = matrix(runif(n*8, 0,1), ncol = 8)
X_2 = cbind(X_12, X_310)
colnames(X_2) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
beta = c(1, 1, 1, 1, 1, 0, 0.5, 0.8, 1.2, 1.5)
Y_2 = c(X_2%*%beta + rnorm(n, 0, 1))
### We'll start with the cpi of the first feature.
full_model_lm = lm(Y~0+X)
reduced_model_lm = lm(Y~0+X[,-1])
N = 500
zero_term = c()
first_term = c()
second_term = c()
third_term = c()
for (i in 1:n){
beta_curr = beta[-1]
beta_curr[1] = (1+rho)*beta_curr[1]
curr_y = rnorm(N, beta_curr%*%X_2[i,-1],sqrt(2-rho^2))
curr_x = rnorm(N, X_2[i,2]*rho, sqrt(1-rho^2))
second_data =  as.data.frame(cbind(curr_x, matrix(rep(X_2[i,-1], N), ncol = 9, byrow = T)))
colnames(second_data) <- c("1", "2", "3", "4", "5", "6", "7","8", "9", "10")
zero_term[i] = 2*mean((curr_y - predict(full_model, X_2[i,]))*
(Y_2[i] - predict(full_model, X_2[i,])))
first_term[i] = mean((curr_y  - predict(full_model, X_2[i,]))^2)
second_term[i] = mean((curr_y -  predict(full_model, newdata = second_data))^2)
third_term[i] = mean((Y_2[i] -  predict(full_model, newdata = second_data))^2)
}
mean(zero_term)
mean(first_term)
mean(second_term)
mean(third_term)
bias_estimation
### In this document, I'll present some results from the large sample illustration
### of exploration1, where we used lm to estimate \hat{y}
library("ggplot2")
setwd("~/Working/Ning/Giles_Project_1/Code/Lu_Implementation/12-15/")
explor2_rho1 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho1.csv"))[,-c(1,6,7)]
explor2_rho3 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho3.csv"))[,-c(1,6,7)]
explor2_rho5 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho5.csv"))[,-c(1,6,7)]
explor2_rho7 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho7.csv"))[,-c(1,6,7)]
explor2_rho9 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho9.csv"))[,-c(1,6,7)]
bias_estimation <- rbind(colMeans(explor2_rho1), colMeans(explor2_rho3),
colMeans(explor2_rho5), colMeans(explor2_rho7),
colMeans(explor2_rho9))
bias_estimation <- cbind(bias_estimation, rowSums(bias_estimation) - 2*bias_estimation[,3] - 2*bias_estimation[,1])
bias_estimation <- cbind(bias_estimation, 3-2*(seq(1,9,2)/10)^2, (seq(1,9,2)/10))
bias_estimation <- data.frame(bias_estimation)
colnames(bias_estimation) <- c("First_Term", "Second_Term", "Third_Term",
"Forth_Term", "Estimate", "True_Value", "rho")
estimation_plot = ggplot(data = bias_estimation, aes(x = rho))   +
geom_line(aes(x = rho, y = Second_Term), color = "#D16103") +
geom_line(aes(x = rho, y = Third_Term), color = "#52854C")  +
geom_line(aes(x = rho, y = Forth_Term), color = "#4E84C4")  +
geom_line(aes(x = rho, y = True_Value), color = "#999999")
estimation_plot = ggplot(data = bias_estimation, aes(x = rho)) +
geom_line(aes(x = rho, y = True_Value), color = "#999999") +
geom_line(aes(x = rho, y = Estimate), color = "#CC79A7")
estimation_plot
### In this document, I'll present some results from the large sample illustration
### of exploration1, where we used lm to estimate \hat{y}
library("ggplot2")
setwd("~/Working/Ning/Giles_Project_1/Code/Lu_Implementation/12-15/")
explor2_rho1 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho1.csv"))[,-c(1,6,7)]
explor2_rho3 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho3.csv"))[,-c(1,6,7)]
explor2_rho5 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho5.csv"))[,-c(1,6,7)]
explor2_rho7 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho7.csv"))[,-c(1,6,7)]
explor2_rho9 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho9.csv"))[,-c(1,6,7)]
bias_estimation <- rbind(colMeans(explor2_rho1), colMeans(explor2_rho3),
colMeans(explor2_rho5), colMeans(explor2_rho7),
colMeans(explor2_rho9))
bias_estimation <- cbind(bias_estimation, rowSums(bias_estimation) - 2*bias_estimation[,3] - 2*bias_estimation[,1])
bias_estimation <- cbind(bias_estimation, 3-2*(seq(1,9,2)/10)^2, (seq(1,9,2)/10))
bias_estimation <- data.frame(bias_estimation)
colnames(bias_estimation) <- c("First_Term", "Second_Term", "Third_Term",
"Forth_Term", "Estimate", "True_Value", "rho")
estimation_plot = ggplot(data = bias_estimation, aes(x = rho))   +
geom_line(aes(x = rho, y = Second_Term), color = "#D16103") +
geom_line(aes(x = rho, y = Third_Term), color = "#52854C")  +
geom_line(aes(x = rho, y = Forth_Term), color = "#4E84C4")  +
geom_line(aes(x = rho, y = True_Value), color = "#999999")
estimation_plot = ggplot(data = bias_estimation, aes(x = rho)) +
geom_line(aes(x = rho, y = True_Value), color = "#999999") +
geom_line(aes(x = rho, y = Estimate), color = "#CC79A7")
################################################################################
################################################################################
################################################################################
t_95 = qt(.975, 2000-1)
estimate_fun <- function(mat){
return(rowSums(mat) - 2*mat[,3] -2* mat[,1])
}
eif_mean = 3-2*seq(from =.1, to = .9, by = .2)^2
var_explor2_rho1 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho1.csv"))[, 7]
var_explor2_rho3 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho3.csv"))[, 7]
var_explor2_rho5 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho5.csv"))[, 7]
var_explor2_rho7 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho7.csv"))[, 7]
var_explor2_rho9 = data.matrix(read.csv("./crossfit_explore_2_2_both_rf_large_rho9.csv"))[, 7]
coverage_mat <- cbind(var_explor2_rho1, var_explor2_rho3, var_explor2_rho5,
var_explor2_rho7, var_explor2_rho9)
estimate_mat <- cbind(estimate_fun(explor2_rho1), estimate_fun(explor2_rho3),
estimate_fun(explor2_rho5), estimate_fun(explor2_rho7),
estimate_fun(explor2_rho9))
SE_vec <- sqrt(colMeans(coverage_mat)/1999)
coverage <- c()
for(i in 1:5){
eif_Lower = eif_mean[i] - t_95*SE_vec[i]/2
eif_Upper = eif_mean[i] + t_95*SE_vec[i]/2
eif_covered =  mean(estimate_mat[,i]<= eif_Upper&estimate_mat[,i]>=eif_Lower)
coverage[i]  = eif_covered
}
coverage
bias_estimation <- cbind(bias_estimation, coverage)
coverage_plot = ggplot(data = bias_estimation, aes(x = rho)) +
geom_line(aes(x = rho, y = coverage), color = "#4E84C4") +
geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") + ylim(0,1)
coverage_plot
1 - pf(5.715841, 4, 35)
pf(5.715841, 4, 35, lower.tail = F)
